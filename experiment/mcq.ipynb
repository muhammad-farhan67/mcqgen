{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json \n",
    "import pandas as pd\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY =os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    api_key = KEY,\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback handler for Groq\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "\n",
    "from langchain.callbacks.base import BaseCallbackHandler#+\n",
    "class GroqCallbackHandler(BaseCallbackHandler):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.total_tokens = 0\n",
    "        self.prompt_tokens = 0\n",
    "        self.completion_tokens = 0\n",
    "        self.total_cost = 0.0\n",
    "        self.encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")  # Using this as a proxy for Groq models\n",
    "\n",
    "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "        print(f\"Starting LLM call...\")\n",
    "        prompt_tokens = sum(len(self.encoding.encode(p)) for p in prompts)\n",
    "        self.prompt_tokens += prompt_tokens\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        if response.generations:\n",
    "            completion_tokens = sum(len(self.encoding.encode(gen.text)) for gen in response.generations[0])\n",
    "            self.completion_tokens += completion_tokens\n",
    "            self.total_tokens = self.prompt_tokens + self.completion_tokens\n",
    "            # Note: Adjust the cost calculation based on Groq's actual pricing\n",
    "            self.total_cost += (self.total_tokens / 1000) * 0.0001  # Assuming $0.0001 per 1k tokens\n",
    "        print(f\"LLM call completed. Total tokens used: {self.total_tokens}\")\n",
    "\n",
    "    def on_chain_start(self, serialized, inputs, **kwargs):\n",
    "        print(f\"Starting chain: {serialized.get('name', 'Unknown')}\")\n",
    "\n",
    "    def on_chain_end(self, outputs, **kwargs):\n",
    "        print(f\"Chain completed. Output keys: {list(outputs.keys())}\")\n",
    "\n",
    "@contextmanager\n",
    "def get_groq_callback():\n",
    "    cb = GroqCallbackHandler()\n",
    "    yield cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from contextlib import contextmanager\n",
    "%pip install tiktoken\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.messages import HumanMessage\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_JSON = {\n",
    "    \"1\":{\n",
    "    \" mcq\":\"Multiple Choice question\",\n",
    "    \"options\":{\n",
    "        \"a\": \"choice here\",\n",
    "        \"b\": \"choice here\",\n",
    "        \"c\": \"choice here\",\n",
    "        \"d\": \"choice here\",\n",
    "        \n",
    "    },\n",
    "    \"correct\":\"correct answer\",\n",
    "},\n",
    "\"2\":{\n",
    "    \"mcq\":\"Multiple Choice question\",\n",
    "    \"options\":{\n",
    "        \"a\": \"choice here\",\n",
    "        \"b\": \"choice here\",\n",
    "        \"c\": \"choice here\",\n",
    "        \"d\": \"choice here\",\n",
    "        \n",
    "    },\n",
    "    \"correct\":\"correct answer\",\n",
    "},\n",
    "\"3\":{\n",
    "    \"mcq\":\"Multiple Choice question\",\n",
    "    \"options\":{\n",
    "        \"a\": \"choice here\",\n",
    "        \"b\": \"choice here\",\n",
    "        \"c\": \"choice here\",\n",
    "        \"d\": \"choice here\",\n",
    "        \n",
    "    },\n",
    "    \"correct\":\"correct answer\",\n",
    "},  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "Text:{text}\n",
    "You are an exper MCQ maker. Given the above text, it is your job to \\\n",
    "create a quiz of {number} multiple choice questions for {subject} students {tone} tone.\n",
    "Make sure the questions are not repeated and check all the questionsto be conforming the text as well.\n",
    "Make sure to format your response like RESPONSE_JSON below as use it as a guide. \\\n",
    "    Ensure to make {number} MCQs\n",
    "    ### RESPONSE_JSON  \n",
    "    \n",
    "    {response_json}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"Text\",\"number\",\"Subject\",\"Tone\",\"response_json\"],\n",
    "    template=TEMPLATE\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_chain=LLMChain(llm=llm,prompt=quiz_generation_prompt,output_key=\"quiz\",verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE2 = \"\"\"\n",
    "You are expert english grammarian and writer . Given a multiple choice quiz for {subject} students. \\\n",
    "    You need to evaluate the complexity of the question and give a complete analysis of the quiz . Only use at max 50 words for complexity.\n",
    "    if the quiz is not at per with the cognitive and analytical abilities of the students,\\\n",
    "    update the quiz questions which needs to be changed and change the tone as it perfectly fits the students ability.\n",
    "    Quiz_MCQs:\n",
    "    {quiz}\n",
    "    \n",
    "    Check from an expert english writer of the above quiz:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_evaluation_prompt=PromptTemplate(input_variables=[\"subject\",\"quiz\"],template=TEMPLATE2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chain=LLMChain(llm=llm,prompt=quiz_evaluation_prompt,output_key=\"review\",verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluate_chain=SequentialChain(chains=[quiz_chain,review_chain],input_variables=[\"text\",\"number\",\"subject\",\"tone\",\"response_json\"],\n",
    "                                        output_variables=[\"quiz\",\"review\"],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=r\"C:\\Users\\DELL\\mcqgen\\data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path,'r') as file:\n",
    "    Text=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(RESPONSE_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER = 5\n",
    "SUBJECT = \"machine learning\"\n",
    "TONE = \"simple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the chain with the custom callback\n",
    "with get_groq_callback() as cb:\n",
    "    response = generate_evaluate_chain({\n",
    "        \"text\": Text,\n",
    "        \"number\": NUMBER,  # Make sure NUMBER is defined\n",
    "        \"subject\": SUBJECT,  # Make sure SUBJECT is defined\n",
    "        \"tone\": TONE,  # Make sure TONE is defined\n",
    "        \"response_json\": json.dumps(RESPONSE_JSON)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results and token usage\n",
    "print(f\"Quiz: {response['quiz']}\")\n",
    "print(f\"Review: {response['review']}\")\n",
    "print(f\"\\nToken Usage:\")\n",
    "print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "print(f\"Total Cost (USD): ${cb.total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz=response.get(\"quiz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz = json.loads(quiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_table_data = []\n",
    "for key, value in quiz.items():\n",
    "    mcq = value[\"mcq\"]\n",
    "    options = \" | \".join(\n",
    "        [\n",
    "            f\"{option}:{option_value}\"\n",
    "            for option,option_value in value[\"options\"].items()\n",
    "        ]\n",
    "    )\n",
    "    correct = value[\"correct\"]\n",
    "    quiz_table_data.append({\"MCQ\":mcq,\"Choices\": options,\"Correct\":correct})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz=pd.DataFrame(quiz_table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz.to_csv(\"machinelearning.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
